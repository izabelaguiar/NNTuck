import tensorly as tl
from tensorly.decomposition._base_decomposition import DecompositionMixin
from tensorly.decomposition import tucker
from tensorly.base import unfold
from tensorly.tenalg import multi_mode_dot, mode_dot
from tensorly.tucker_tensor import tucker_to_tensor, TuckerTensor, validate_tucker_rank
from tensorly.random import check_random_state
import tensorly.tenalg as tlg
import matplotlib.pyplot as plt
from math import sqrt
import numpy as np
import warnings
import copy
tl.set_backend('numpy')

def max_like(tensor, core, factors, masked = False, M = None):
    B = tucker_to_tensor((core, factors), transpose_factors= False)
    if masked:
        B = M * B
    mask = B > 0
    return np.sum(np.where(mask, tensor*np.log(B, where=mask)-B, 0))

def KL_D(tensor, core, factors, masked = False, M = None):
    mask = tensor >0
    B = tucker_to_tensor((core, factors), transpose_factors= False)
    if masked:
        B = M * B
    B_p = copy.deepcopy(B)
    B_p[np.where(B<1e-6)] = 1e-6
    return np.sum(np.where(mask, tensor*np.log(tensor/B_p, where=mask) -tensor +B, 0))+np.sum(np.where(tensor==0, B, 0))

def GenerateSBM(N, G1, Omega, TYPE, G2, theta = None, undirected =  True, Poisson = True, unweighted = True):
    '''
    
    inputs:
        N: number of nodes in the network
        G: (N x K) group membership matrix
        Omega: (K x K) connectivity matrix
        TYPE: 'reg', 'dc', 'o', 'mm'
        theta: (N x 1) vector. Expected degree of node i. Only necessary for 'dc'
    
    outputs: 
        A: (N x N) adjacency matrix for network generated by SBM
        
    '''
    if (TYPE != 'reg' and TYPE != 'dc' and TYPE != 'o' and TYPE != 'mm'):
        print('Oops, TYPE must be one of "reg" "dc" "o" or "mm". Quitting.')
        return 0
    
    A = np.zeros((N, N))
    
    if not Poisson:
        def f(p):
            return np.random.binomial(1, p, 1)[0]
    if Poisson:
        def f(p):
            return np.random.poisson(p)
    
    if TYPE == 'reg' or TYPE == 'o':
        if undirected:
            for i in range(N):
                gi = np.where(G1[i, :] == 1)[0][0]
                for j in range(i): 
                    gj = np.where(G1[j, :] == 1)[0][0]
                    A[i, j] = f(Omega[gi, gj])
                    A[j, i] = A[i, j]
            return A
    
    if TYPE == 'dc':
        if undirected:
            if len(theta) != N:
                print('Input "theta" must be a vector of length N.')
                return 0
            for i in range(N):
                gi = np.where(G[i, :] == 1)[0][0]
                for j in range(i):
                    gj = np.where(G1[j, :] == 1)[0][0]
                    print('p= ',theta[i]*theta[j]*Omega[gi, gj])
                    A[i, j] = f(theta[i]*theta[j]*Omega[gi, gj])
                    A[j, i] = A[i, j]
        return A
    
    if TYPE == 'mm':
        M = G1@Omega@G2.T
        if undirected:
            for i in range(N):
                for j in range(i):
                    A[i, j] = f(M[i,j])
                    A[j, i] = A[i, j]
        else:
            for i in range(N):
                for j in range(N):
                    A[i, j] = f(M[i,j])
                    #print(A[i,j])
        if unweighted:
            for i in range(N):
                for j in range(N):
                    if A[i, j] > 1:
                        A[i, j] = 1
        return A

def D(A, W, H):
    #print("in D")
    B = W@H
    S = 0
    N, N = np.shape(A)
    for i in range(N):
        for j in range(N):
            if A[i, j] != 0:
                den = max(B[i, j], 1e-7)
                #print("den is ", den)
                S += A[i, j]* np.log(A[i, j]/den) - A[i, j] + B[i, j]
            else:
                S += B[i, j]
    return S

def non_negative_tucker(tensor, rank, n_iter_max=10, init='svd', tol=10e-5, MT_stopping_conditions = False, symmetric = False,
                        random_state=None, verbose=False, loss = 'KL', constrained=False, init_core =None, init_factors =None, 
                        returnErrors = False, return_fact_it = False, masked = False, Masking = None):
    """Non-negative Tucker decomposition
        Iterative multiplicative update, see [2]_
        Updated 2/10/21 by Izzy to include option for multiplicative updates for KL-loss
    Parameters
    ----------
    tensor : ``ndarray``
    rank : None, int or int list
        size of the core tensor, ``(len(ranks) == tensor.ndim)``
        if int, the same rank is used for all modes
    n_iter_max : int
        maximum number of iteration
    init : {'svd', 'random'}
    tol : stopping conditions
    MT_stopping_conditions : {True, False}
        if True, uses the stopping conditions from MULTITENSOR paper.          ### REMOVE IF PUSHING TO TENSORLY ###
    symmetric : {True, False}
        if True, enforces U=V          ### REMOVE IF PUSHING TO TENSORLY ###
    random_state : {None, int, np.random.RandomState}
    verbose : int , optional
        level of verbosity
    loss : string, optional
        Which multiplicative update to use. 'KL' or 'LS'.
    ranks : None or int list
    size of the core tensor
    Returns
    -------
    core : ndarray
            positive core of the Tucker decomposition
            has shape `ranks`
    factors : ndarray list
            list of factors of the CP decomposition
            element `i` is of shape ``(tensor.shape[i], rank)``
    References
    ----------
    .. [2] Yong-Deok Kim and Seungjin Choi,
       "Non-negative tucker decomposition",
       IEEE Conference on Computer Vision and Pattern Recognition s(CVPR),
       pp 1-8, 2007
    """
    M = Masking #just to help notation
    rank = validate_tucker_rank(tl.shape(tensor), rank=rank)
    if loss != 'KL' and loss != 'LS':
        print('loss must be one of KL or LS')
        return
    if MT_stopping_conditions:
        EM_count = 0;
    epsilon = 10e-12
    I_l = np.shape(tensor)[0]
    
    if masked:
        tensor = M * tensor # hereafter, everytime 'tensor' is referenced, it'll only be the observed entries.
        #then we don't have to change the numerator for any of the updates at all. 
        M_unfold = []
        for mode in range(tl.ndim(tensor)):
            M_unfold.append(unfold(M, mode))
                
    # Initialisation
    if init == 'svd':
        core, factors = tucker(tensor, rank)
        nn_factors = [tl.abs(f) for f in factors]
        if constrained: nn_factors[0] = np.eye(I_l)
        if symmetric: nn_factors[2] = copy.deepcopy(nn_factors[1])
        if symmetric and np.allclose(nn_factors[2], nn_factors[1]) and verbose: 
            print('yay, U==V')
        elif symmetric and not np.allclose(nn_factors[2], nn_factors[1]) and verbose:
            print('oh no, U!=V')
        nn_core = tl.abs(core)
    if init == 'custom':
        if init_core is None or init_factors is None:
            print('ERROR: For custom initialization, must provide init_core and init_factors')
            return
        else:
            nn_core = copy.deepcopy(init_core)
            nn_factors = copy.deepcopy(init_factors)
            #print('init factors[0] is {}'.format(init_factors[0]))
        if constrained: 
            nn_factors[0] = np.eye(I_l)
            print('doing constrained')
        if symmetric: nn_factors[2] = copy.deepcopy(nn_factors[1])
        if symmetric and np.allclose(nn_factors[2], nn_factors[1]) and verbose: 
            print('yay, U==V')
        elif symmetric and not np.allclose(nn_factors[2], nn_factors[1]) and verbose:
            print('oh no, U!=V')
    else:
        rng = check_random_state(random_state)
        core = tl.tensor(rng.random_sample(rank) + 0.01, **tl.context(tensor))  
        factors = [tl.tensor(rng.random_sample(s), **tl.context(tensor)) for s in zip(tl.shape(tensor), rank)]
        nn_factors = [tl.abs(f) for f in factors]
        if constrained: 
            nn_factors[0] = np.eye(I_l)
        if symmetric: nn_factors[2] = copy.deepcopy(nn_factors[1])
        if symmetric and np.allclose(nn_factors[2], nn_factors[1]) and verbose: 
            print('yay, U==V')
        elif symmetric and not np.allclose(nn_factors[2], nn_factors[1]):
            print('oh no, U!=V')
        nn_core = tl.abs(core)
    if symmetric:
        for alpha in range(rank[0]):
            nn_core[alpha ,:, :] = nn_core[alpha ,:, :]@nn_core[alpha ,:, :].T
            if np.allclose(nn_core[alpha ,:, :],nn_core[alpha ,:, :].T) and verbose:
                print('yay, core is now symmetric')

    norm_tensor = tl.norm(tensor, 2)
    rec_errors = []
    EM_errors = []
    if return_fact_it:
        facts = []
        facts.append((copy.deepcopy(nn_core), copy.deepcopy(nn_factors)))
    if loss == 'KL':
        if not masked:
            rec_errors.append(KL_D(tensor, nn_core, nn_factors))
        else:
            rec_errors.append(KL_D(tensor, nn_core, nn_factors, masked, M))
    else: 
        if not masked:
            rec_error = tl.norm(tensor - tucker_to_tensor((nn_core, nn_factors)), 2) / norm_tensor
        else:
            rec_error = tl.norm(tensor - M * tucker_to_tensor((nn_core, nn_factors)), 2) / norm_tensor
        rec_errors.append(rec_error)
    if rec_errors[0]<tol:
        print('{} loss is {} upon initialization. Already converged.'.format(loss, rec_errors[0]))
        return nn_core, nn_factors
    if not masked:
        EM_errors.append(max_like(tensor, nn_core, nn_factors))
    else:
        EM_errors.append(max_like(tensor, nn_core, nn_factors, masked, M))
        
    for iteration in range(n_iter_max):

        for mode in range(tl.ndim(tensor)):
            I_n = np.shape(tensor)[mode]
            if constrained and mode==0:
                nn_factors[mode] = np.eye(I_n)
            elif (not symmetric) or (symmetric and mode == 1): 
                if loss == 'KL':
                    B = tucker_to_tensor((nn_core, nn_factors), skip_factor=mode)
                    B = unfold(B, mode)
                    numerator = unfold(tensor, mode)
                    numerator = numerator / tl.clip(tl.dot(nn_factors[mode], B), a_min=epsilon, a_max=None)
                    numerator = tl.dot(numerator, tl.transpose(B))
                    numerator = tl.clip(numerator, a_min=epsilon, a_max=None)
                    if not masked:
                        z = np.sum(B, axis=1)
                        denominator = np.outer(np.ones(I_n), z)
                    else:
                        denominator = tl.dot(M_unfold[mode],tl.transpose(B))
                    denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
                else: #haven't "masked" this.
                    B = tucker_to_tensor((nn_core, nn_factors), skip_factor=mode)
                    B = tl.transpose(unfold(B, mode))
                    numerator = tl.dot(unfold(tensor, mode), B)
                    numerator = tl.clip(numerator, a_min=epsilon, a_max=None)
                    denominator = tl.dot(nn_factors[mode], tl.dot(tl.transpose(B), B))
                    denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
                
                nn_factors[mode] *= numerator / denominator
                if symmetric and mode == 1:
                    nn_factors[2] *= numerator / denominator

        if symmetric and iteration %50 ==0 and verbose: 
            print('It is {} at iteration {} that , U==V'.format(np.allclose(nn_factors[2], nn_factors[1]), iteration))
                    
        if loss == 'KL':
            X_hat = tucker_to_tensor((nn_core, nn_factors), transpose_factors= False)
            X_hat = tl.clip(X_hat, a_min = epsilon, a_max = None)
            numerator = tucker_to_tensor((tensor/X_hat, nn_factors), transpose_factors= True)
            if masked:
                E = M
            else:
                E = np.ones_like(tensor)
            denominator = tucker_to_tensor((E, nn_factors), transpose_factors= True)
            denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
        else: #haven't "masked" this.
            numerator = tucker_to_tensor((tensor, nn_factors), transpose_factors=True)
            numerator = tl.clip(numerator, a_min=epsilon, a_max=None)
            for i, f in enumerate(nn_factors):
                if i:
                    denominator = mode_dot(denominator, tl.dot(tl.transpose(f), f), i)
                else:
                    denominator = mode_dot(nn_core, tl.dot(tl.transpose(f), f), i)
            denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
        
        mul_update = numerator / denominator
        if symmetric and iteration %50 ==0 and verbose:
            print('It is {} that mul_update[0] is symmetric at iteration {}'.format(np.allclose(mul_update[0], mul_update[0].T), iteration))
        nn_core *= numerator / denominator
        
        if return_fact_it:
            facts.append((copy.deepcopy(nn_core), copy.deepcopy(nn_factors)))
        if loss == 'KL':
            if not masked:
                rec_errors.append(KL_D(tensor, nn_core, nn_factors))
            else:
                rec_errors.append(KL_D(tensor, nn_core, nn_factors, masked, M))
        else:
            rec_error = tl.norm(tensor - tucker_to_tensor((nn_core, nn_factors)), 2) / norm_tensor
            rec_errors.append(rec_error)
            
        if not masked:
            EM_errors.append(max_like(tensor, nn_core, nn_factors))  
        else:
            EM_errors.append(max_like(tensor, nn_core, nn_factors, masked, M))
            
        if iteration % 50 ==0 and verbose:
            print('iteration {}: {} loss = {}, variation = {}.'.format(iteration, loss,
                rec_errors[-1], rec_errors[-2] - rec_errors[-1]))
            
        if iteration > 0 and rec_errors[-1]>rec_errors[-2]:
            print("ERROR: Loss increasing at iteration ", iteration)
        
        if not MT_stopping_conditions:
            if iteration > 1 and abs(rec_errors[-2] - rec_errors[-1])/abs(rec_errors[-1]) < tol:
                if verbose:
                    print('converged in {} iterations. {} loss = {}'.format(iteration, loss, rec_errors[-1]))
                break
        else: 
            if iteration > 1 and abs(EM_errors[-2] - EM_errors[-1])/abs(EM_errors[-1]) < tol:
                EM_count += 1
                if EM_count == 10:
                    if verbose:
                        print('converged in {} iterations. {} loss = {}'.format(iteration, loss, rec_errors[-1]))
                        print('maximum likelihood = {}'.format(EM_errors[-1]))
                    break
                    
    if not returnErrors:
        return TuckerTensor((nn_core, nn_factors))
    else:
        if return_fact_it:
            return TuckerTensor((nn_core, nn_factors)), EM_errors, rec_errors, facts
        else:
            return TuckerTensor((nn_core, nn_factors)), EM_errors, rec_errors


def non_negative_tucker_ones(tensor, rank, n_iter_max=10, init='svd', tol=10e-5, MT_stopping_conditions = False, symmetric = False,
                        random_state=None, verbose=False, loss = 'KL', constrained=True, init_core =None, init_factors =None, 
                        returnErrors = False, return_fact_it = False, masked = False, Masking = None):
    """Non-negative Tucker decomposition
        Iterative multiplicative update, see [2]_
        Constrained so that Y== np.ones((C))
        Updated 2/10/21 by Izzy to include option for multiplicative updates for KL-loss
    Parameters
    ----------
    tensor : ``ndarray``
    rank : None, int or int list
        size of the core tensor, ``(len(ranks) == tensor.ndim)``
        if int, the same rank is used for all modes
    n_iter_max : int
        maximum number of iteration
    init : {'svd', 'random'}
    tol : stopping conditions
    MT_stopping_conditions : {True, False}
        if True, uses the stopping conditions from MULTITENSOR paper.          ### REMOVE IF PUSHING TO TENSORLY ###
    symmetric : {True, False}
        if True, enforces U=V          ### REMOVE IF PUSHING TO TENSORLY ###
    random_state : {None, int, np.random.RandomState}
    verbose : int , optional
        level of verbosity
    loss : string, optional
        Which multiplicative update to use. 'KL' or 'LS'.
    ranks : None or int list
    size of the core tensor
    Returns
    -------
    core : ndarray
            positive core of the Tucker decomposition
            has shape `ranks`
    factors : ndarray list
            list of factors of the CP decomposition
            element `i` is of shape ``(tensor.shape[i], rank)``
    References
    ----------
    .. [2] Yong-Deok Kim and Seungjin Choi,
       "Non-negative tucker decomposition",
       IEEE Conference on Computer Vision and Pattern Recognition s(CVPR),
       pp 1-8, 2007
    """
    M = Masking #just to help notation
    rank = validate_tucker_rank(tl.shape(tensor), rank=rank)
    if loss != 'KL' and loss != 'LS':
        print('loss must be one of KL or LS')
        return
    if MT_stopping_conditions:
        EM_count = 0;
    epsilon = 10e-12
    I_l = np.shape(tensor)[0]
    
    if masked:
        tensor = M * tensor # hereafter, everytime 'tensor' is referenced, it'll only be the observed entries.
        #then we don't have to change the numerator for any of the updates at all. 
        M_unfold = []
        for mode in range(tl.ndim(tensor)):
            M_unfold.append(unfold(M, mode))
                
    # Initialisation
    if init == 'svd':
        core, factors = tucker(tensor, rank)
        nn_factors = [tl.abs(f) for f in factors]
        if constrained: nn_factors[0] = np.ones((I_l,1))
        if symmetric: nn_factors[2] = copy.deepcopy(nn_factors[1])
        if symmetric and np.allclose(nn_factors[2], nn_factors[1]) and verbose: 
            print('yay, U==V')
        else:
            print('oh no, U!=V')
        nn_core = tl.abs(core)
    if init == 'custom':
        if init_core is None or init_factors is None:
            print('ERROR: For custom initialization, must provide init_core and init_factors')
            return
        else:
            nn_core = init_core
            nn_factors = init_factors
    else:
        rng = check_random_state(random_state)
        core = tl.tensor(rng.random_sample(rank) + 0.01, **tl.context(tensor))  
        factors = [tl.tensor(rng.random_sample(s), **tl.context(tensor)) for s in zip(tl.shape(tensor), rank)]
        nn_factors = [tl.abs(f) for f in factors]
        if constrained: nn_factors[0] = np.ones((I_l,1))
        if symmetric: nn_factors[2] = copy.deepcopy(nn_factors[1])
        if symmetric and np.allclose(nn_factors[2], nn_factors[1]) and verbose: 
            print('yay, U==V')
        elif symmetric and not np.allclose(nn_factors[2], nn_factors[1]):
            print('oh no, U!=V')
        nn_core = tl.abs(core)
    if symmetric:
        for alpha in range(rank[0]):
            nn_core[alpha ,:, :] = nn_core[alpha ,:, :]@nn_core[alpha ,:, :].T
            if np.allclose(nn_core[alpha ,:, :],nn_core[alpha ,:, :].T) and verbose:
                print('yay, core is now symmetric')

    norm_tensor = tl.norm(tensor, 2)
    rec_errors = []
    EM_errors = []
    if return_fact_it:
        facts = []
    if loss == 'KL':
        if not masked:
            rec_errors.append(KL_D(tensor, nn_core, nn_factors))
        else:
            rec_errors.append(KL_D(tensor, nn_core, nn_factors, masked, M))
    else: 
        if not masked:
            rec_error = tl.norm(tensor - tucker_to_tensor((nn_core, nn_factors)), 2) / norm_tensor
        else:
            rec_error = tl.norm(tensor - M * tucker_to_tensor((nn_core, nn_factors)), 2) / norm_tensor
        rec_errors.append(rec_error)
    if rec_errors[0]<tol:
        print('{} loss is {} upon initialization. Already converged.'.format(loss, rec_errors[0]))
        return nn_core, nn_factors
    if not masked:
        EM_errors.append(max_like(tensor, nn_core, nn_factors))
    else:
        EM_errors.append(max_like(tensor, nn_core, nn_factors, masked, M))
        
    for iteration in range(n_iter_max):

        for mode in range(tl.ndim(tensor)):
            I_n = np.shape(tensor)[mode]
            if constrained and mode==0:
                nn_factors[mode] = np.ones((I_n,1))
            elif (not symmetric) or (symmetric and mode == 1): 
                if loss == 'KL':
                    B = tucker_to_tensor((nn_core, nn_factors), skip_factor=mode)
                    B = unfold(B, mode)
                    numerator = unfold(tensor, mode)
                    numerator = numerator / tl.clip(tl.dot(nn_factors[mode], B), a_min=epsilon, a_max=None)
                    numerator = tl.dot(numerator, tl.transpose(B))
                    numerator = tl.clip(numerator, a_min=epsilon, a_max=None)
                    if not masked:
                        z = np.sum(B, axis=1)
                        denominator = np.outer(np.ones(I_n), z)
                    else:
                        denominator = tl.dot(M_unfold[mode],tl.transpose(B))
                    denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
                else: #haven't "masked" this.
                    B = tucker_to_tensor((nn_core, nn_factors), skip_factor=mode)
                    B = tl.transpose(unfold(B, mode))
                    numerator = tl.dot(unfold(tensor, mode), B)
                    numerator = tl.clip(numerator, a_min=epsilon, a_max=None)
                    denominator = tl.dot(nn_factors[mode], tl.dot(tl.transpose(B), B))
                    denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
                
                nn_factors[mode] *= numerator / denominator
                if symmetric and mode == 1:
                    nn_factors[2] *= numerator / denominator

        if symmetric and iteration %50 ==0 and verbose: 
            print('It is {} at iteration {} that , U==V'.format(np.allclose(nn_factors[2], nn_factors[1]), iteration))
                    
        if loss == 'KL':
            X_hat = tucker_to_tensor((nn_core, nn_factors), transpose_factors= False)
            X_hat = tl.clip(X_hat, a_min = epsilon, a_max = None)
            numerator = tucker_to_tensor((tensor/X_hat, nn_factors), transpose_factors= True)
            if masked:
                E = M
            else:
                E = np.ones_like(tensor)
            denominator = tucker_to_tensor((E, nn_factors), transpose_factors= True)
            denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
        else: #haven't "masked" this.
            numerator = tucker_to_tensor((tensor, nn_factors), transpose_factors=True)
            numerator = tl.clip(numerator, a_min=epsilon, a_max=None)
            for i, f in enumerate(nn_factors):
                if i:
                    denominator = mode_dot(denominator, tl.dot(tl.transpose(f), f), i)
                else:
                    denominator = mode_dot(nn_core, tl.dot(tl.transpose(f), f), i)
            denominator = tl.clip(denominator, a_min=epsilon, a_max=None)
        
        mul_update = numerator / denominator
        if symmetric and iteration %50 ==0 and verbose:
            print('It is {} that mul_update[0] is symmetric at iteration {}'.format(np.allclose(mul_update[0], mul_update[0].T), iteration))
        nn_core *= numerator / denominator
        if return_fact_it:
            facts.append((copy.deepcopy(nn_core),  copy.deepcopy(nn_factors)))
        if loss == 'KL':
            if not masked:
                rec_errors.append(KL_D(tensor, nn_core, nn_factors))
            else:
                rec_errors.append(KL_D(tensor, nn_core, nn_factors, masked, M))
        else:
            rec_error = tl.norm(tensor - tucker_to_tensor((nn_core, nn_factors)), 2) / norm_tensor
            rec_errors.append(rec_error)
            
        if not masked:
            EM_errors.append(max_like(tensor, nn_core, nn_factors))  
        else:
            EM_errors.append(max_like(tensor, nn_core, nn_factors, masked, M))
            
        if iteration % 50 ==0 and verbose:
            print('iteration {}: {} loss = {}, variation = {}.'.format(iteration, loss,
                rec_errors[-1], rec_errors[-2] - rec_errors[-1]))
            
        if iteration > 0 and rec_errors[-1]>rec_errors[-2]:
            print("ERROR: Loss increasing at iteration ", iteration)
        
        if not MT_stopping_conditions:
            if iteration > 1 and abs(rec_errors[-2] - rec_errors[-1])/abs(rec_errors[-1]) < tol:
                if verbose:
                    print('converged in {} iterations. {} loss = {}'.format(iteration, loss, rec_errors[-1]))
                break
        else: 
            if iteration > 1 and abs(EM_errors[-2] - EM_errors[-1])/abs(EM_errors[-1]) < tol:
                EM_count += 1
                if EM_count == 10:
                    if verbose:
                        print('converged in {} iterations. {} loss = {}'.format(iteration, loss, rec_errors[-1]))
                        print('maximum likelihood = {}'.format(EM_errors[-1]))
                    break
                    
    if not returnErrors:
        return TuckerTensor((nn_core, nn_factors))
    else:
        if return_fact_it:
            return TuckerTensor((nn_core, nn_factors)), EM_errors, rec_errors, facts
        else:
            return TuckerTensor((nn_core, nn_factors)), EM_errors, rec_errors
        
def Y_interp(factors, ystar = False, r_star = None):
    Y = factors[0]
    ALPHA, C = np.shape(Y)
    if ystar:
        Y_star = np.zeros_like(Y)
        r_bar = [r for r in range(12) if r not in r_star]
        Y_rstar = Y[r_star, :]
        Y_rbar = Y[r_bar, :]
        Y_star[r_star, :] = np.eye(C)
        Y_star[r_bar, :] = np.linalg.solve(Y_rstar.T, Y_rbar.T).T
    Y_2norm = Y/np.linalg.norm(Y, axis = 1, keepdims = True)
    plt.imshow(Y_2norm@Y_2norm.T, cmap = 'binary')
    with np.printoptions(precision = 4, suppress=True):
        print("True Y is ")
        print("")
        print(Y)
        print("")
        print("Y_norm is")
        print("")
        print(Y/np.sum(Y, axis = 1, keepdims = True))
        print("")
        if ystar:
            print("row normalized Y_star is")
            print("")
            print(Y_star/np.sum(abs(Y_star), axis = 1, keepdims = True))
            print("")
